# Install required libraries
!pip install pandas spacy umap-learn bertopic sentence-transformers

# Download the Swedish spaCy model
!python -m spacy download sv_core_news_sm


from google.colab import files
uploded = files.upload()


import pandas as pd
import re
import spacy
from umap import UMAP
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt

# -------------------- 1. Load Data --------------------
# Load the CSV files you uploaded
df_r = pd.read_csv('top_20_posters_reddit.csv')
df_f = pd.read_csv('top_20_posters_flashback.csv')


# Add a source column to each DataFrame to keep track of the origin
df_r['source'] = 'top_20_posters_reddit'
df_f['source'] = 'top_20_posters_flashback'

# Concatenate data from all files
df = pd.concat([df_r, df_f], ignore_index=True)

# Drop rows with missing 'post' column data
df.dropna(subset=['posts'], inplace=True)

# -------------------- 2. Preprocess & Normalize Text --------------------

nlp = spacy.load("sv_core_news_sm")


# Define custom stop words (you can add any word here you want to exclude)
custom_stop_words = ['just', 'fr', 'och', 'att', 'den', 'det', 'som', 'är', 'inte','vara','p', 'ju','se','väl','ca','tro']


def preprocess_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+", "", text)  # Remove URLs
    text = re.sub(r"[^a-zåäö0-9\s]", "", text)  # Remove non-alphanumeric characters
    doc = nlp(text)
    tokens = [
        token.lemma_ for token in doc
        if not token.is_punct and not token.is_space and
        token.lemma_ not in custom_stop_words and
        not token.is_stop
    ]
    return " ".join(tokens)
df["processed_content"] = df["posts"].apply(preprocess_text)

# -------------------- 3. Tokenize & Reconstruct Texts --------------------
df["tokens"] = df["processed_content"].apply(lambda x: x.split())
df["reconstructed_text"] = df["tokens"].apply(lambda x: " ".join(x))

# -------------------- 4. Define BERTopic --------------------
umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)

# Load the Swedish SentenceTransformer model
embedding_model = SentenceTransformer('KBLab/sentence-bert-swedish-cased')

# Initialize BERTopic
topic_model = BERTopic(
    umap_model=umap_model,
    min_topic_size=35, #adjusts if needed
    embedding_model=embedding_model
)

# -------------------- 5. Fit BERTopic to Data --------------------
docs = df["reconstructed_text"].tolist()
topics, probs = topic_model.fit_transform(docs)

# -------------------- 6. Visualize & Interpret Topics --------------------
# Add the topic column to the DataFrame
df["topic"] = topics

# Print Topic Information
print(topic_model.get_topic_info())

# Visualize the top 10 topics in a bar chart
topic_model.visualize_barchart(top_n_topics=10)

# -------------------- 7. Visualize Topic Distribution by Source --------------------
# Group the data by source and topic to count the distribution
topic_counts = df.groupby(['source', 'topic']).size().unstack(fill_value=0)

# Plot the distribution of topics for each source
topic_counts.plot(kind='bar', stacked=True, figsize=(10, 6))
plt.title('Topic Distribution by Source')
plt.xlabel('Source')
plt.ylabel('Number of Posts')
plt.legend(title='Topic')
plt.xticks(rotation=0)
plt.show()

# -------------------- 8. Save Topics to CSV --------------------
df.to_csv("topic_model_results.csv", index=False)


# **Displaying Representative Documents for Each Topic**

# Get topic information
topic_info = topic_model.get_topic_info()

# Iterate through topics and display representative documents
for index, row in topic_info.iterrows():
    topic_num = row['Topic']
    if topic_num != -1:  # Exclude the '-1' topic (outliers or noise)
        print(f"Topic {topic_num}: {row['Name']}")
        representative_docs = topic_model.get_representative_docs(topic_num)
        for doc in representative_docs:
            print(f"- {doc}")
        print("\n")


topic_names = {
    -1: 'Unclassified',  # if you have -1 for unassigned topics
     0: 'systemic-driven',
     1: 'Party-driven'
}

topic_counts = topic_counts.rename(columns=topic_names)

topic_counts.plot(kind='bar', stacked=True, figsize=(10, 6))
plt.title('Topic Distribution: Comparing Flashback and Reddit')
plt.xlabel('Platform')
plt.ylabel('Number of Posts')
plt.legend(title='Topic')
plt.xticks(rotation=0)
plt.show()



import pandas as pd
from scipy.stats import chi2_contingency

# Assuming topic_counts is a DataFrame with topic counts for each row
systemic = topic_counts['systemic-driven']
totals = topic_counts.sum(axis=1)

# Create the contingency table
contingency_table = pd.DataFrame({
    'Mentions systemic-driven': systemic,
    'Does not mention': totals - systemic
}, index=topic_counts.index)

# Perform the Chi-square test
chi2, p, dof, expected = chi2_contingency(contingency_table)

# Print the results
print(f"Chi² = {chi2:.2f}, p = {p:.4f}")


# Add percentage column
percentages = (systemic / totals * 100).round(2)
print(percentages)


import numpy as np

# Chi-square statistic (from previous calculation)
chi2 = 27.73

# Total number of observations (sum of all values in the contingency table)
n = contingency_table.sum().sum()

# Number of rows and columns (2x2 table, so k = r = 2)
k = r = 2

# Calculate Cramér's V
cramers_v = np.sqrt(chi2 / (n * (min(k - 1, r - 1))))
print(f"Cramér's V: {cramers_v:.4f}")



import matplotlib.pyplot as plt

# Calculate percentages of "Systemic cheating" mentions
percentages = (systemic / totals * 100).round(2)

# Plot the percentages
plt.figure(figsize=(8, 5))
percentages.plot(kind='bar', color=['darkblue', 'darkred'])
plt.title('Percentage of Posts Blaming Systemic Factors by Platform')
plt.xlabel('Platform')
plt.ylabel('Percentage of Posts')
plt.xticks(rotation=0)
plt.ylim(0, 100)  # To ensure the y-axis starts at 0 and ends at 100%
plt.show()

# Visualize the top 10 topics in a bar chart
topic_model.visualize_barchart(top_n_topics=10)


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency

# Assuming `topic_counts` is your DataFrame and 'systemic-driven' is the column of interest
systemic = topic_counts['systemic-driven']
totals = topic_counts.sum(axis=1)

# Creating the contingency table with percentages
contingency_table = pd.DataFrame({
    'Mentions System Fraud': (systemic / totals) * 100,  # Percentage of mentions
    'Does not mention': ((totals - systemic) / totals) * 100  # Percentage of non-mentions
}, index=topic_counts.index)

# Plotting the table as a heatmap with percentages
plt.figure(figsize=(8, 6))
sns.heatmap(contingency_table, annot=True, cmap="Blues", fmt='.2f', cbar=False, annot_kws={'size': 12})

# Add labels and title
plt.title('Contingency Table: Percentages of Mentions and Non-Mentions of System Fraud')
plt.xlabel('Categories')
plt.ylabel('Topics')

# Display the plot
plt.tight_layout()
plt.show()

# Perform the Chi-squared test
chi2, p, dof, expected = chi2_contingency(contingency_table)

# Create a DataFrame to display the Chi-squared test results
test_results = pd.DataFrame({
    'Chi² Statistic': [chi2],
    'p-value': [p],
    'Degrees of Freedom': [dof]
})

# Create a DataFrame for the expected frequencies (rounded to 2 decimals)
expected_df = pd.DataFrame(expected, columns=contingency_table.columns, index=contingency_table.index)

# Display the results
print("Chi-squared Test Results:")
print(test_results)

print("\nExpected Frequencies:")
print(expected_df)

# Optional: Plotting the test results table and expected frequencies
# Plotting the test results
plt.figure(figsize=(8, 2))
sns.heatmap(test_results, annot=True, cmap="Blues", fmt='.4f', cbar=False, annot_kws={'size': 12})
plt.title('Chi-squared Test Results')
plt.tight_layout()
plt.show()

# Plotting the expected frequencies table
plt.figure(figsize=(8, 6))
sns.heatmap(expected_df, annot=True, cmap="YlGnBu", fmt='.2f', cbar=False, annot_kws={'size': 12})
plt.title('Expected Frequencies')
plt.tight_layout()
print(expected_df)


import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# -------------------- 1. Load the CSV file --------------------
df = pd.read_csv("topic_model_results.csv")

# -------------------- 2. Create a Word Cloud for Each Topic --------------------
# Drop rows with missing values or fill them with empty strings
df = df.dropna(subset=['processed_content'])  # Or df['processed_content'] = df['processed_content'].fillna('')

# Ensure 'processed_content' column contains only strings
df['processed_content'] = df['processed_content'].astype(str)

# Group the data by topics and combine the texts from each topic
grouped = df.groupby('topic')['processed_content'].apply(' '.join).reset_index()

# -------------------- 3. Generate and Display Word Clouds for Each Topic --------------------
for topic_num, text in zip(grouped['topic'], grouped['processed_content']):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Topic {topic_num}", fontsize=16)
    plt.show()



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import Isomap
from umap import UMAP
from bertopic import BERTopic

# -------------------- 1. Load the CSV File --------------------
df = pd.read_csv("topic_model_results.csv")

# -------------------- 2. Get Topic Probabilities --------------------
# You can obtain the topic probabilities for each document
topic_probabilities = df["topic"].values

# -------------------- 3. Apply Isomap or UMAP for Dimensionality Reduction --------------------
# Use UMAP (or Isomap) to reduce the dimensionality to 2D
umap_model = UMAP(n_components=2)  # Reduce to 2D for visualization
reduced_embeddings = umap_model.fit_transform(topic_probabilities.reshape(-1, 1))

# -------------------- 4. Visualize Topics in 2D --------------------
plt.figure(figsize=(10, 6))

# Create a scatter plot
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=topic_probabilities, cmap='viridis', s=100)

# Add topic labels for clarity
for i, topic in enumerate(topic_probabilities):
    plt.annotate(topic, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=9)

plt.title('Topic Distances Using UMAP')
plt.xlabel('UMAP Component 1')
plt.ylabel('UMAP Component 2')
plt.colorbar(label='Topic ID')
plt.show()
